{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proyecto_JPMuñoz.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "kCO9_qul3dAh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Text Mining: Proyecto semestral\n",
        "\n",
        "Juan Pablo Muñoz\n",
        "\n",
        "martes, 15 de enero del 2019"
      ]
    },
    {
      "metadata": {
        "id": "RcwOTLPy3wMa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Introducción\n",
        "\n",
        "En este *notebook* se resumen los experimentos y avances logrados en el desarrollo del proyecto semestral del curso *Text Mining* 2018-2. El proyecto tiene por objetivo aplicar los conocimientos adquiridos durante el curso para proponer una solución a algún problema dentro o fuera del área, con algún elemento novedoso.\n",
        "\n",
        "Este proyecto propone una serie de métodos y algoritmos para la creación de vectores de palabras basados en Word2vec que no sufran del problema de la *fusión de significados*\n",
        "\n",
        "La *fusión de significados* es una propiedad indeseada que se manifiesta en las representaciones vectoriales que se basan en Word2vec. En su formato original, Word2vec intenta generar un espacio vectorial en el que cada palabra observada pueda ser representada, de manera que, en el nuevo espacio, ésta quede cerca de las palabras que tienden a aparecer en su cercanía, a la vez que queda lejos de palabras que tienden a no aparecer cerca o en una misma frase. Pero este modelo de representación no considera dos propiedades muy importantes de la semántica: 1) que las palabras pueden tener varios significados y 2) que el significado de una palabra en uso depende fuertemente de las palabras que la rodean (contexto). Esta limitación del diseño de Word2vec causa que el modelo, por ejemplo, sólo produzca una única representación para la palabra \"ratón\", cuando en realidad \"ratón\" puede referirse a un animal mamífero o a un dispositivo tecnológico. Al mismo tiempo, el modelo se esforzará por hacer que todas las palabras fuertemente relacionadas a \"ratón (animal)\" y \"ratón (tecnología)\" queden cerca de la representación fusionada de \"ratón\", por lo que se deshace la propiedad distributiva del espacio vectorial resultante.\n"
      ]
    },
    {
      "metadata": {
        "id": "QIR-TSTc_FbC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Metodología de Trabajo\n",
        "\n",
        "El proceso de creación de vectores Word2vec tiene las siguientes etapas:\n",
        "\n",
        " - Obtención de un corpus, que es un conjunto de documentos de texto\n",
        " - Preprocesar el corpus\n",
        " - Crear y entrenar un modelo Word2vec sobre el corpus preprocesado\n",
        " \n",
        "Para poner en uso el modelo de representación creado, debe existir una consulta. Esta consulta es una cadena de texto, que debe ser preprocesada de la misma manera que el corpus antes de crear el modelo, para luego ser transformada al espacio de representación del modelo, donde se puede realizar la tarea que se requiera (por ejemplo, recuperación de información o resolución de preguntas, entre otros).\n",
        " \n",
        "** La etapa más importante, costosa y complicada en este proyecto resulta ser el de preprocesamiento de texto**. Fue experimentando aquí donde se concentró todo el esfuerzo y tiempo del proyecto.\n",
        "\n",
        "Luego de lograda esa parte, se puede entrenar un modelo word2vec para realizar *tests* de coherencia interna del espacio vectorial y para realizar *downstream tasks*. Sin embargo, estas actividades sólo sirven para el propósito de validación y no se les dio prioridad en esta iteración del trabajo.\n",
        "\n",
        "## Datos\n",
        "\n",
        "El corpus utilizado fue el depósito general de artículos de la Wikipedia en inglés: [enwiki-latest-pages-articles-multistream.xml.bz](https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles-multistream.xml.bz2) (tamaño ~15 GB con ~4.5 millones de artículos). Para los experimentos realizados, se utilizó una muestra aleatoria de un ~10% de los artículos.\n",
        "\n",
        "Los métodos y algoritmos desarrollados utilizan fuentes de conocimiento léxico externas al corpus de texto utilizado para entrenar. En esta iteración del trabajo, se utiliza WordNet y la interfaz de NLTK para tener acceso simplificado a las estructuras de datos léxicas y a las operaciones realizables sobre ellas.\n",
        "\n",
        "## Descripción del Preprocesado\n",
        "\n",
        "Nota: Debido al alto costo computacional que tiene (de momento) decidir a priori si una palabra es o no ambigua, y la desambiguación implementada, se requiere de la indicación explícita de aquellas palabras a desambiguar durante el preprocesado del corpus.\n",
        "\n",
        "A continuación, se describe el preprocesado del corpus, dada una lista de palabras ambiguas. Los nombres y argumentos descritos aquí no corresponden a los implementados. Esta modificación se hizo para priorizar rapidez de escritura y facilidad de entendimiento. La descripción detallada de las funciones se encuentra en cada implementación.\n",
        "\n",
        "```\n",
        "def preprocesado(corpus, lista_palabras_ambiguas):\n",
        "  corpus_preprocesado = list()\n",
        "  for documento in corpus:\n",
        "    tokenizar(documento)\n",
        "    remover_stopwords(documento)\n",
        "    lematizar(documento)\n",
        "    hacer_part_of_speech_tag(documento)\n",
        "    desambiguar(documento, lista_palabras_ambiguas)\n",
        "    corpus_preprocesado.append(documento)\n",
        "  return corpus_preprocesado\n",
        "```\n",
        "\n",
        "Del anterior proceso, es necesario explicar `desambiguar()`:\n",
        "\n",
        "```\n",
        "def desambiguar(documento, lista_palabras_ambiguas):\n",
        "  for palabra in documento:\n",
        "    if palabra in lista_palabras_ambiguas:\n",
        "      contexto = ventana(documento, palabra)\n",
        "      palabra_desambiguada = decidir_significado(palabra, contexto)\n",
        "      reemplazar(documento, palabra, palabra_desambiguada)\n",
        "    return documento\n",
        "```\n",
        "\n",
        "Donde, `decidir_significado()` es una función que accede a los diccionarios de WordNet y permite decidir el significado más probable de una palabra dado su contexto, en base a múltiples métricas de similitud definidas sobre las estructuras de datos de WordNet llamadas `Synsets`, que se comportan como grafos y son utilizadas para representar relaciones jerárquicas entre términos relacionados, definiciones, ejemplos, etc."
      ]
    },
    {
      "metadata": {
        "id": "fLAf3ZVYNHXW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Código"
      ]
    },
    {
      "metadata": {
        "id": "DyR_ShUuNLMV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Clase y métodos para cargar el corpus de Wikipedia haciendo primeros pasos de preprocesado\n",
        "\n",
        "Cargar el archivo `bz2` del corpus de la Wikipedia con una clase especialmente diseñada en la librería **gensim**. Esta clase permite realizar tokenización, transformación a letras minúsculas, remoción de *stop words*, lematización y *part-of-speech tagging* al momento de la carga de cada artículo. De manera adicional, se implementa y aplica sobre cada artículo una función normalizadora `remove_accents()`, que transforma los carácteres alfabéticos tildados a su forma original (ej.: \"árbol\"->\"arbol\")."
      ]
    },
    {
      "metadata": {
        "id": "xdlK-PHfMyuN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "\n",
        "import time\n",
        "import logging\n",
        "import os.path\n",
        "import sys\n",
        "import random\n",
        "\n",
        "import unicodedata\n",
        "\n",
        "def remove_accents(input_str):\n",
        "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
        "    only_ascii = nfkd_form.encode('ASCII', 'ignore')\n",
        "    return only_ascii.decode('utf-8')\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "lmtzr = nltk.WordNetLemmatizer().lemmatize\n",
        "\n",
        "def get_wordnet_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return wordnet.NOUN\n",
        "\n",
        "\n",
        "def lemmatize_text(text):\n",
        "    word_pos = nltk.pos_tag(nltk.word_tokenize(text))\n",
        "    lemm_words = [lmtzr(sw[0], get_wordnet_pos(sw[1])) for sw in word_pos]\n",
        "\n",
        "    return [x.lower() for x in lemm_words]\n",
        "\n",
        "'''\n",
        "class gensim.corpora.wikicorpus.WikiCorpus(\n",
        "    fname,\n",
        "    processes=None, \n",
        "    lemmatize=True, \n",
        "    dictionary=None, \n",
        "    filter_namespaces=('0', ), \n",
        "    tokenizer_func=<function tokenize>, \n",
        "    article_min_tokens=50, \n",
        "    token_min_len=2, \n",
        "    token_max_len=15, \n",
        "    lower=True, \n",
        "    filter_articles=None\n",
        ")\n",
        "\n",
        "'''\n",
        "\n",
        "from gensim.corpora import WikiCorpus\n",
        "\n",
        "class WikiCorpusLoader:\n",
        "\n",
        "    def __init__(self, wiki_dump_file, just_lemmatize=False, pos=False, dictionary={}):\n",
        "        self.wiki = WikiCorpus(wiki_dump_file, lemmatize=pos, dictionary=dictionary)\n",
        "        program = os.path.basename(sys.argv[0])\n",
        "        self.logger = logging.getLogger(program)\n",
        "        self.lemmatize = just_lemmatize\n",
        "        self.pos = pos\n",
        "        logging.basicConfig(format='%(asctime)s: %(levelname)s: %(message)s')\n",
        "        logging.root.setLevel(level=logging.INFO)\n",
        "        self.logger.info(\"running %s\" % ' '.join(sys.argv))\n",
        "        \n",
        "        \n",
        "    def generate_sample(self, output_file, sample_frac=0.01, random_state_seed=99):\n",
        "        assert sample_frac > 0 and sample_frac <= 1\n",
        "        #assert not os.path.exists(output_file)\n",
        "        output_no_lemma = open(output_file+'_no_lemma.txt', 'w')\n",
        "        if self.lemmatize:\n",
        "            output_lemma = open(output_file+'_lemma.txt', 'w')\n",
        "        if self.pos:\n",
        "            output_pos = open(output_file+'_pos.txt', 'w')\n",
        "        i = 0\n",
        "        idx = 0\n",
        "        period = 100000\n",
        "        if self.pos:\n",
        "            period = 2000\n",
        "        random.seed(random_state_seed)\n",
        "        for article in self.wiki.get_texts():\n",
        "            idx += 1\n",
        "            if sample_frac < 1:\n",
        "                article_selected = random.random() <= sample_frac\n",
        "                if not article_selected:\n",
        "                    continue\n",
        "            else:\n",
        "                i = i + 1\n",
        "                if not self.pos:\n",
        "                    normalized_article = bytes(' '.join(remove_accents(w) for w in article), 'utf-8').decode('utf-8')\n",
        "                else:\n",
        "                    normalized_article = ' '.join(remove_accents(w.decode('utf-8')) for w in article)\n",
        "                output_no_lemma.write(str(idx)+':'+normalized_article+'\\n')\n",
        "                if self.lemmatize:\n",
        "                    output_lemma.write(str(idx)+':'+' '.join(lemmatize_text(normalized_article))+'\\n')\n",
        "                if self.pos:\n",
        "                    output_pos.write(str(idx)+'\\n')\n",
        "                \n",
        "                if (i % period == 0):\n",
        "                    self.logger.info(\"Saved \" + str(i) + \" articles\")\n",
        "        output_no_lemma.close()\n",
        "        if self.lemmatize:\n",
        "            output_lemma.close()\n",
        "        if self.pos:\n",
        "            output_pos.close()\n",
        "        self.logger.info(\"Finished saving \" + str(i) + \" articles.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sg98z8g0O98E",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Funciones para la desambiguación de un corpus preprocesado\n",
        "\n",
        "Estas funciones permiten iterar sobre los documentos preprocesados, realizando desambiguación automática de todas las palabras que se hayan indicado. Luego de este paso, el corpus queda listo para ser usado en el entrenamiento de modelos Word2vec."
      ]
    },
    {
      "metadata": {
        "id": "LFrZ2qAtoy-f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "093c6ee0-2430-4431-99a7-bd7296292874"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "def get_similarity(ss1, ss2, method='path'):\n",
        "  if method =='path':\n",
        "    return max(\n",
        "        s for s in [\n",
        "            0,\n",
        "            wn.path_similarity(ss1,ss2,simulate_root=False),\n",
        "            wn.path_similarity(ss2,ss1,simulate_root=False),\n",
        "        ] if s is not None\n",
        "    )\n",
        "  if method =='wup':\n",
        "    return max(\n",
        "        s for s in [\n",
        "            0,\n",
        "            wn.wup_similarity(ss1,ss2,simulate_root=False),\n",
        "            wn.wup_similarity(ss2,ss1,simulate_root=False),\n",
        "        ] if s is not None\n",
        "    )\n",
        "\n",
        "\n",
        "def perform_deambiguation(\n",
        "    document,\n",
        "    target_word_index,\n",
        "    method='path',\n",
        "    verbose=False,\n",
        "):\n",
        "  \n",
        "  '''\n",
        "  deambiguates a word given a context, by comparing all WordNet synsets of the\n",
        "  target word with those of the words in the context\n",
        "  word: 2-tuple (<token>, <pos_tag>)\n",
        "  context: list of 2-tuples [(<token>, <pos_tag>), ...]\n",
        "  \n",
        "  Method: Choose the meaning that maximizes the sum of similarities between it\n",
        "  and the most similar sense of every word in the context.\n",
        "  \n",
        "  Intuitively: Select the meaning that \"makes the more sense\" possible given the\n",
        "  most convenient interpretation of every word in the context.\n",
        "  \n",
        "  Note: currently using path-based similarity -> Higher similarity when score is\n",
        "  higher.\n",
        "  '''\n",
        "  \n",
        "  assert target_word_index < len(document) and 0 <= target_word_index\n",
        "  \n",
        "  word = document[target_word_index]\n",
        "  context_pre = document[0:target_word_index]\n",
        "  context_post = document[target_word_index+1:]\n",
        "  context = context_pre+context_post\n",
        "  \n",
        "  # Base case: if the target word is not in WordNet, return the document\n",
        "  # unmodified\n",
        "  candidate_meanings = wn.synsets(word[0], pos=word[1])\n",
        "  if not candidate_meanings:\n",
        "    return list(w[0] for w in document)\n",
        "  if verbose:\n",
        "    print('Context:',context)\n",
        "    for candidate_meaning in candidate_meanings:\n",
        "      print('Target:',candidate_meaning.name())\n",
        "      for context_word, context_pos in context:\n",
        "        for context_word_synset in wn.synsets(context_word, pos=context_pos):\n",
        "          print('similarity({}, {})=\\t\\t\\t{}'.format(\n",
        "              candidate_meaning.name(),\n",
        "              context_word_synset.name(),\n",
        "              get_similarity(candidate_meaning, context_word_synset, method)\n",
        "          ))\n",
        "\n",
        "  scores = dict()\n",
        "  # Iterate over all possible meanings of the word to be deambiguated\n",
        "  for candidate_meaning in wn.synsets(word[0], pos=word[1]):\n",
        "    \n",
        "    # Evaluate \"how much sense\" makes the candidate meaning with the context\n",
        "    similarities = ([\n",
        "        get_similarity(candidate_meaning, context_word_synset, method) \\\n",
        "        for context_word_synset in wn.synsets(context_word, pos=context_pos)\n",
        "    ] for context_word, context_pos in context)\n",
        "    scores[candidate_meaning] = sum(max(s) for s in similarities)\n",
        "  \n",
        "  # Choose the meaning that makes \"more sense\"\n",
        "  result = max(scores, key=scores.get)\n",
        "  \n",
        "  # Strip PoS tags off since they are no longer needed\n",
        "  tagless_context_pre = []\n",
        "  tagless_context_post = []\n",
        "  if len(context_pre) > 0:\n",
        "    tagless_context_pre = [w for w, tag in context_pre]\n",
        "  if len(context_post) > 0:\n",
        "    tagless_context_post = [w for w, tag in context_post]\n",
        "    \n",
        "  # Reconstruct document with the deambiguated target word\n",
        "  result_doc = tagless_context_pre+[result.name()]+tagless_context_post\n",
        "  return result_doc\n",
        "\n",
        "def penn_to_wn(pos_tag):\n",
        "    if pos_tag.startswith('J'):\n",
        "        return wn.ADJ\n",
        "    elif pos_tag.startswith('N'):\n",
        "        return wn.NOUN\n",
        "    elif pos_tag.startswith('R'):\n",
        "        return wn.ADV\n",
        "    elif pos_tag.startswith('V'):\n",
        "        return wn.VERB\n",
        "    return None\n",
        "\n",
        "def penn_to_wn_document(doc):\n",
        "  wn_doc = []\n",
        "  for token, pos_tag in doc:\n",
        "    wn_doc.append((token, penn_to_wn(pos_tag)))\n",
        "  return wn_doc\n",
        "  \n",
        "def pattern_to_wn_pos_transform(document):\n",
        "  \n",
        "  '''\n",
        "  ['token_1/POS_1', ...] -> [('token_1', 'POS_1'), ...]\n",
        "  '''\n",
        "  \n",
        "  r = [(t.split('/')[0], t.split('/')[1]) for t in document]\n",
        "  return r\n",
        "  \n",
        "def deambiguate_article(\n",
        "    document, \n",
        "    word_list, \n",
        "    pos_tagged=False, \n",
        "    method='path',\n",
        "    window_size=2,\n",
        "    verbose=False,\n",
        "):\n",
        "  \n",
        "  '''\n",
        "  Processes a document, first by stripping any stop word off it, then by\n",
        "  performing a PoS tagging for specific words to be deambiguated and their \n",
        "  contexts, of size window_size. After the PoS tagging, tries to deambiguate the\n",
        "  target word.\n",
        "  \n",
        "  document: list of word strings\n",
        "  word_list: words to be PoS tagged\n",
        "  pos_tagged: True if document comes already PoS tagged (format: 'token/POS')\n",
        "  method: 'path' for path-based similarity measure. 'ic' for \n",
        "  information-content-based similarity measure (TODO: support for 'ic')\n",
        "  window_size: number of words before and after a target word to be considered\n",
        "  as its context. Ideally, a bigger window should improve the quality of the PoS\n",
        "  tagging and deambiguation until certain point (too big windows sizes could \n",
        "  span more than one context, phrase or paragraph).\n",
        "  '''\n",
        "  if not pos_tagged:\n",
        "    stopwords_eng = stopwords.words('english')\n",
        "    document = [token for token in document if token not in stopwords_eng]\n",
        "  deambiguated_doc = ['']*len(document)\n",
        "  for index, w in enumerate(document):\n",
        "    if deambiguated_doc[index] != '':\n",
        "      continue\n",
        "    tagless_w = w.split('/')[0]\n",
        "    deambiguated_doc[index] = tagless_w\n",
        "    if tagless_w in word_list:\n",
        "      window_start = max(0, index-window_size)\n",
        "      window_end = min(len(document), index+window_size+1)\n",
        "      target_w_index = index-window_start\n",
        "      window = document[window_start:window_end]\n",
        "      if pos_tagged:\n",
        "        pos_tagged_window = pattern_to_wn_pos_transform(window)\n",
        "        pos_tagged_window = penn_to_wn_document(pos_tagged_window)\n",
        "      else:\n",
        "        pos_tagged_window = penn_to_wn_document(nltk.pos_tag(window))\n",
        "      deambiguated_w = perform_deambiguation(\n",
        "          pos_tagged_window, \n",
        "          target_w_index,\n",
        "          method=method,\n",
        "          verbose=verbose,\n",
        "      )\n",
        "      deambiguated_doc[window_start:window_end] = deambiguated_w\n",
        "  return deambiguated_doc"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HK6nvLRVVRZm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Lectura del corpus"
      ]
    },
    {
      "metadata": {
        "id": "-373USQVQMdc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Lectura de corpus\n",
        "\n",
        "%%time\n",
        "\n",
        "print(\"Cargando y procesando corpus de Wikipedia...\")\n",
        "corpus_loader = WikiCorpusLoader(\n",
        "    wiki_dump_file='data/enwiki-latest-pages-articles-multistream.xml.bz2',\n",
        "    just_lemmatize=False,\n",
        "    pos=True\n",
        ")\n",
        "\n",
        "corpus_loader.generate_sample(\n",
        "    output_file='data/wikicorpus_0.1',\n",
        "    sample_frac=0.1,\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jJHkj0vJRB8F",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Salida de la celda de lectura de corpus**:\n",
        "\n",
        "2019-01-09 19:57:56,101: INFO: running c:\\program files\\python36\\lib\\site-packages\\ipykernel_launcher.py -f C:\\Users\\Juampiblo\\AppData\\Roaming\\jupyter\\runtime\\kernel-baa5de1c-3e6f-46dc-8875-cd40d4c3abe0.json\n",
        "\n",
        "Cargando muestra del corpus de Wikipedia...\n",
        "\n",
        "2019-01-09 20:02:45,123: INFO: Saved 1000 articles\n",
        "\n",
        "2019-01-09 20:06:29,344: INFO: Saved 2000 articles\n",
        "\n",
        "2019-01-09 20:09:38,782: INFO: Saved 3000 articles\n",
        "\n",
        "...\n",
        "\n",
        "2019-01-10 11:44:28,385: INFO: Saved 456000 articles\n",
        "\n",
        "2019-01-10 11:45:24,977: INFO: Saved 457000 articles\n",
        "\n",
        "2019-01-10 11:46:24,737: INFO: Saved 458000 articles\n",
        "\n",
        "2019-01-10 11:47:15,700: INFO: finished iterating over Wikipedia corpus of \n",
        "4583951 documents with 2556203780 positions (total 19096287 articles, \n",
        "2624561277 positions before pruning articles shorter than 50 words)\n",
        "2019-01-10 11:47:16,063: INFO: Finished saving 458949 articles.\n",
        "Wall time: 15h 49min 19s"
      ]
    },
    {
      "metadata": {
        "id": "c3jo2MjFShIt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Muestra del corpus cargado\n",
        "\n",
        "Luego de la lectura del corpus, se produce un archivo de texto plano que contiene un artículo preprocesado por línea. Si se lee un artículo en una lista dividida por espacios (`split()`), el formato de cada artículo es el siguiente:"
      ]
    },
    {
      "metadata": {
        "id": "82AWXtr5nIh_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "3ef5f5ec-cd31-45f7-d864-b728ddd614ae"
      },
      "cell_type": "code",
      "source": [
        "# Muestra de uno de los artículos del corpus y su formato final\n",
        "\n",
        "a_sub = [b'anarchism/NN', b'be/VB', b'political/JJ', b'philosophy/NN', b'advocate/VB', b'self/NN', b'govern/VB', b'society/NN', b'base/VB', b'voluntary/JJ', b'cooperative/JJ', b'institution/NN', b'reject/VB', b'unjust/JJ', b'hierarchy/NN', b'institution/NN', b'be/VB', b'often/RB', b'describe/VB', b'stateless/JJ', b'society/NN', b'several/JJ', b'author/NN', b'have/VB', b'define/VB', b'more/RB', b'specifically/RB', b'institution/NN', b'base/VB', b'hierarchical/JJ', b'free/JJ', b'association/NN', b'anarchism/NN', b'hold/VB', b'capitalism/NN', b'state/NN']\n",
        "a_sub = [w.decode('utf-8') for w in a_sub]\n",
        "a_sub"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anarchism/NN',\n",
              " 'be/VB',\n",
              " 'political/JJ',\n",
              " 'philosophy/NN',\n",
              " 'advocate/VB',\n",
              " 'self/NN',\n",
              " 'govern/VB',\n",
              " 'society/NN',\n",
              " 'base/VB',\n",
              " 'voluntary/JJ',\n",
              " 'cooperative/JJ',\n",
              " 'institution/NN',\n",
              " 'reject/VB',\n",
              " 'unjust/JJ',\n",
              " 'hierarchy/NN',\n",
              " 'institution/NN',\n",
              " 'be/VB',\n",
              " 'often/RB',\n",
              " 'describe/VB',\n",
              " 'stateless/JJ',\n",
              " 'society/NN',\n",
              " 'several/JJ',\n",
              " 'author/NN',\n",
              " 'have/VB',\n",
              " 'define/VB',\n",
              " 'more/RB',\n",
              " 'specifically/RB',\n",
              " 'institution/NN',\n",
              " 'base/VB',\n",
              " 'hierarchical/JJ',\n",
              " 'free/JJ',\n",
              " 'association/NN',\n",
              " 'anarchism/NN',\n",
              " 'hold/VB',\n",
              " 'capitalism/NN',\n",
              " 'state/NN']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "metadata": {
        "id": "4KsKLAIITNbW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Desambiguación\n",
        "\n",
        "A continuación, se realiza el último paso del preprocesado, que es la desambiguación.\n",
        "\n",
        "Para desambiguar una palabra, se debe\n",
        "\n",
        " - Entregar un contexto (lista de palabras) que acompaña a la palabra a desambiguar. Recordar que el sentido de una palabra está fuertemente relacionado a las palabras de su contexto.\n",
        " - Entregar lematizados y PoS-taggeados tanto la palabra como el contexto. La lematización es un paso necesario para llevar todas las variaciones o conjugaciones de las palabras a su forma base, y permite identificarlas independientemente cómo se usen. El *PoS-tagging* permite a los métodos desamgibuadores reducir la cantidad inicial de significados posibles de una palabra ambigua. Si el *PoS-tagger* ha identificado a la palabra \"light\" como un adjetivo dado el contexto en el que se observó, entonces los métodos desambiguadores desambiguarán solamente considerando los significados de \"light\" que sean adjetivos entre los candidatos.\n",
        " \n",
        " La salida de la siguiente celda muestra el proceso comparativo que realizan los métodos desambiguadores sobre todos los significados candidatos de cada palabra ambigua (presente en la lista de palabras ambiguas `word_list`) encontrada.\n",
        " \n",
        "Cuando se encuentra una palabra a desambiguar en el artículo, se cuantifica \"cuánto sentido\" tiene cada posible significado de la misma con las palabras de su contexto. Para esto, se comparan los `synsets` del significado candidato con todos los `synsets` de todas las palabras del contexto. Con `n` significados candidatos, dado un contexto de `m` palabras con `o` `synsets` cada una, la cantidad de veces que se aplica la medida de similitud entre `synsets` está en el orden `O(n*m*o)`."
      ]
    },
    {
      "metadata": {
        "id": "sE7MvSDm3J5w",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 9081
        },
        "outputId": "fde265f3-d33a-43d7-e1c9-3e7ef6383b66"
      },
      "cell_type": "code",
      "source": [
        "%%time\n",
        "\n",
        "a_sub_deambiguated = deambiguate_article(\n",
        "    document=a_sub, \n",
        "    word_list=['institution', 'govern', 'have', 'define'], \n",
        "    pos_tagged=True, \n",
        "    window_size=2, \n",
        "    verbose=True\n",
        ")"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context: [('advocate', 'v'), ('self', 'n'), ('society', 'n'), ('base', 'v')]\n",
            "Target: regulate.v.02\n",
            "similarity(regulate.v.02, recommend.v.01)=\t\t\t0\n",
            "similarity(regulate.v.02, preach.v.02)=\t\t\t0\n",
            "similarity(regulate.v.02, self.n.01)=\t\t\t0\n",
            "similarity(regulate.v.02, self.n.02)=\t\t\t0\n",
            "similarity(regulate.v.02, society.n.01)=\t\t\t0\n",
            "similarity(regulate.v.02, club.n.02)=\t\t\t0\n",
            "similarity(regulate.v.02, company.n.03)=\t\t\t0\n",
            "similarity(regulate.v.02, society.n.04)=\t\t\t0\n",
            "similarity(regulate.v.02, establish.v.08)=\t\t\t0\n",
            "similarity(regulate.v.02, base.v.02)=\t\t\t0\n",
            "similarity(regulate.v.02, free-base.v.01)=\t\t\t0\n",
            "Target: govern.v.02\n",
            "similarity(govern.v.02, recommend.v.01)=\t\t\t0\n",
            "similarity(govern.v.02, preach.v.02)=\t\t\t0\n",
            "similarity(govern.v.02, self.n.01)=\t\t\t0\n",
            "similarity(govern.v.02, self.n.02)=\t\t\t0\n",
            "similarity(govern.v.02, society.n.01)=\t\t\t0\n",
            "similarity(govern.v.02, club.n.02)=\t\t\t0\n",
            "similarity(govern.v.02, company.n.03)=\t\t\t0\n",
            "similarity(govern.v.02, society.n.04)=\t\t\t0\n",
            "similarity(govern.v.02, establish.v.08)=\t\t\t0\n",
            "similarity(govern.v.02, base.v.02)=\t\t\t0\n",
            "similarity(govern.v.02, free-base.v.01)=\t\t\t0\n",
            "Target: govern.v.03\n",
            "similarity(govern.v.03, recommend.v.01)=\t\t\t0\n",
            "similarity(govern.v.03, preach.v.02)=\t\t\t0\n",
            "similarity(govern.v.03, self.n.01)=\t\t\t0\n",
            "similarity(govern.v.03, self.n.02)=\t\t\t0\n",
            "similarity(govern.v.03, society.n.01)=\t\t\t0\n",
            "similarity(govern.v.03, club.n.02)=\t\t\t0\n",
            "similarity(govern.v.03, company.n.03)=\t\t\t0\n",
            "similarity(govern.v.03, society.n.04)=\t\t\t0\n",
            "similarity(govern.v.03, establish.v.08)=\t\t\t0\n",
            "similarity(govern.v.03, base.v.02)=\t\t\t0\n",
            "similarity(govern.v.03, free-base.v.01)=\t\t\t0\n",
            "Target: govern.v.04\n",
            "similarity(govern.v.04, recommend.v.01)=\t\t\t0\n",
            "similarity(govern.v.04, preach.v.02)=\t\t\t0\n",
            "similarity(govern.v.04, self.n.01)=\t\t\t0\n",
            "similarity(govern.v.04, self.n.02)=\t\t\t0\n",
            "similarity(govern.v.04, society.n.01)=\t\t\t0\n",
            "similarity(govern.v.04, club.n.02)=\t\t\t0\n",
            "similarity(govern.v.04, company.n.03)=\t\t\t0\n",
            "similarity(govern.v.04, society.n.04)=\t\t\t0\n",
            "similarity(govern.v.04, establish.v.08)=\t\t\t0\n",
            "similarity(govern.v.04, base.v.02)=\t\t\t0\n",
            "similarity(govern.v.04, free-base.v.01)=\t\t\t0\n",
            "Context: [('voluntary', 'a'), ('cooperative', 'a'), ('reject', 'v'), ('unjust', 'a')]\n",
            "Target: institution.n.01\n",
            "similarity(institution.n.01, voluntary.a.01)=\t\t\t0\n",
            "similarity(institution.n.01, voluntary.a.02)=\t\t\t0\n",
            "similarity(institution.n.01, concerted.s.01)=\t\t\t0\n",
            "similarity(institution.n.01, cooperative.a.02)=\t\t\t0\n",
            "similarity(institution.n.01, accommodative.s.02)=\t\t\t0\n",
            "similarity(institution.n.01, reject.v.01)=\t\t\t0\n",
            "similarity(institution.n.01, refuse.v.02)=\t\t\t0\n",
            "similarity(institution.n.01, disapprove.v.02)=\t\t\t0\n",
            "similarity(institution.n.01, reject.v.04)=\t\t\t0\n",
            "similarity(institution.n.01, resist.v.05)=\t\t\t0\n",
            "similarity(institution.n.01, reject.v.06)=\t\t\t0\n",
            "similarity(institution.n.01, rule_out.v.03)=\t\t\t0\n",
            "similarity(institution.n.01, unfair.a.01)=\t\t\t0\n",
            "similarity(institution.n.01, unjust.a.02)=\t\t\t0\n",
            "similarity(institution.n.01, inequitable.a.01)=\t\t\t0\n",
            "Target: institution.n.02\n",
            "similarity(institution.n.02, voluntary.a.01)=\t\t\t0\n",
            "similarity(institution.n.02, voluntary.a.02)=\t\t\t0\n",
            "similarity(institution.n.02, concerted.s.01)=\t\t\t0\n",
            "similarity(institution.n.02, cooperative.a.02)=\t\t\t0\n",
            "similarity(institution.n.02, accommodative.s.02)=\t\t\t0\n",
            "similarity(institution.n.02, reject.v.01)=\t\t\t0\n",
            "similarity(institution.n.02, refuse.v.02)=\t\t\t0\n",
            "similarity(institution.n.02, disapprove.v.02)=\t\t\t0\n",
            "similarity(institution.n.02, reject.v.04)=\t\t\t0\n",
            "similarity(institution.n.02, resist.v.05)=\t\t\t0\n",
            "similarity(institution.n.02, reject.v.06)=\t\t\t0\n",
            "similarity(institution.n.02, rule_out.v.03)=\t\t\t0\n",
            "similarity(institution.n.02, unfair.a.01)=\t\t\t0\n",
            "similarity(institution.n.02, unjust.a.02)=\t\t\t0\n",
            "similarity(institution.n.02, inequitable.a.01)=\t\t\t0\n",
            "Target: institution.n.03\n",
            "similarity(institution.n.03, voluntary.a.01)=\t\t\t0\n",
            "similarity(institution.n.03, voluntary.a.02)=\t\t\t0\n",
            "similarity(institution.n.03, concerted.s.01)=\t\t\t0\n",
            "similarity(institution.n.03, cooperative.a.02)=\t\t\t0\n",
            "similarity(institution.n.03, accommodative.s.02)=\t\t\t0\n",
            "similarity(institution.n.03, reject.v.01)=\t\t\t0\n",
            "similarity(institution.n.03, refuse.v.02)=\t\t\t0\n",
            "similarity(institution.n.03, disapprove.v.02)=\t\t\t0\n",
            "similarity(institution.n.03, reject.v.04)=\t\t\t0\n",
            "similarity(institution.n.03, resist.v.05)=\t\t\t0\n",
            "similarity(institution.n.03, reject.v.06)=\t\t\t0\n",
            "similarity(institution.n.03, rule_out.v.03)=\t\t\t0\n",
            "similarity(institution.n.03, unfair.a.01)=\t\t\t0\n",
            "similarity(institution.n.03, unjust.a.02)=\t\t\t0\n",
            "similarity(institution.n.03, inequitable.a.01)=\t\t\t0\n",
            "Target: initiation.n.02\n",
            "similarity(initiation.n.02, voluntary.a.01)=\t\t\t0\n",
            "similarity(initiation.n.02, voluntary.a.02)=\t\t\t0\n",
            "similarity(initiation.n.02, concerted.s.01)=\t\t\t0\n",
            "similarity(initiation.n.02, cooperative.a.02)=\t\t\t0\n",
            "similarity(initiation.n.02, accommodative.s.02)=\t\t\t0\n",
            "similarity(initiation.n.02, reject.v.01)=\t\t\t0\n",
            "similarity(initiation.n.02, refuse.v.02)=\t\t\t0\n",
            "similarity(initiation.n.02, disapprove.v.02)=\t\t\t0\n",
            "similarity(initiation.n.02, reject.v.04)=\t\t\t0\n",
            "similarity(initiation.n.02, resist.v.05)=\t\t\t0\n",
            "similarity(initiation.n.02, reject.v.06)=\t\t\t0\n",
            "similarity(initiation.n.02, rule_out.v.03)=\t\t\t0\n",
            "similarity(initiation.n.02, unfair.a.01)=\t\t\t0\n",
            "similarity(initiation.n.02, unjust.a.02)=\t\t\t0\n",
            "similarity(initiation.n.02, inequitable.a.01)=\t\t\t0\n",
            "Target: mental_hospital.n.01\n",
            "similarity(mental_hospital.n.01, voluntary.a.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, voluntary.a.02)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, concerted.s.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, cooperative.a.02)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, accommodative.s.02)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, reject.v.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, refuse.v.02)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, disapprove.v.02)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, reject.v.04)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, resist.v.05)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, reject.v.06)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, rule_out.v.03)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, unfair.a.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, unjust.a.02)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, inequitable.a.01)=\t\t\t0\n",
            "Context: [('unjust', 'a'), ('hierarchy', 'n'), ('be', 'v'), ('often', 'r')]\n",
            "Target: institution.n.01\n",
            "similarity(institution.n.01, unfair.a.01)=\t\t\t0\n",
            "similarity(institution.n.01, unjust.a.02)=\t\t\t0\n",
            "similarity(institution.n.01, inequitable.a.01)=\t\t\t0\n",
            "similarity(institution.n.01, hierarchy.n.01)=\t\t\t0.125\n",
            "similarity(institution.n.01, hierarchy.n.02)=\t\t\t0.3333333333333333\n",
            "similarity(institution.n.01, be.v.01)=\t\t\t0\n",
            "similarity(institution.n.01, be.v.02)=\t\t\t0\n",
            "similarity(institution.n.01, be.v.03)=\t\t\t0\n",
            "similarity(institution.n.01, exist.v.01)=\t\t\t0\n",
            "similarity(institution.n.01, be.v.05)=\t\t\t0\n",
            "similarity(institution.n.01, equal.v.01)=\t\t\t0\n",
            "similarity(institution.n.01, constitute.v.01)=\t\t\t0\n",
            "similarity(institution.n.01, be.v.08)=\t\t\t0\n",
            "similarity(institution.n.01, embody.v.02)=\t\t\t0\n",
            "similarity(institution.n.01, be.v.10)=\t\t\t0\n",
            "similarity(institution.n.01, be.v.11)=\t\t\t0\n",
            "similarity(institution.n.01, be.v.12)=\t\t\t0\n",
            "similarity(institution.n.01, cost.v.01)=\t\t\t0\n",
            "similarity(institution.n.01, frequently.r.01)=\t\t\t0\n",
            "similarity(institution.n.01, much.r.05)=\t\t\t0\n",
            "similarity(institution.n.01, often.r.03)=\t\t\t0\n",
            "Target: institution.n.02\n",
            "similarity(institution.n.02, unfair.a.01)=\t\t\t0\n",
            "similarity(institution.n.02, unjust.a.02)=\t\t\t0\n",
            "similarity(institution.n.02, inequitable.a.01)=\t\t\t0\n",
            "similarity(institution.n.02, hierarchy.n.01)=\t\t\t0.07142857142857142\n",
            "similarity(institution.n.02, hierarchy.n.02)=\t\t\t0.07692307692307693\n",
            "similarity(institution.n.02, be.v.01)=\t\t\t0\n",
            "similarity(institution.n.02, be.v.02)=\t\t\t0\n",
            "similarity(institution.n.02, be.v.03)=\t\t\t0\n",
            "similarity(institution.n.02, exist.v.01)=\t\t\t0\n",
            "similarity(institution.n.02, be.v.05)=\t\t\t0\n",
            "similarity(institution.n.02, equal.v.01)=\t\t\t0\n",
            "similarity(institution.n.02, constitute.v.01)=\t\t\t0\n",
            "similarity(institution.n.02, be.v.08)=\t\t\t0\n",
            "similarity(institution.n.02, embody.v.02)=\t\t\t0\n",
            "similarity(institution.n.02, be.v.10)=\t\t\t0\n",
            "similarity(institution.n.02, be.v.11)=\t\t\t0\n",
            "similarity(institution.n.02, be.v.12)=\t\t\t0\n",
            "similarity(institution.n.02, cost.v.01)=\t\t\t0\n",
            "similarity(institution.n.02, frequently.r.01)=\t\t\t0\n",
            "similarity(institution.n.02, much.r.05)=\t\t\t0\n",
            "similarity(institution.n.02, often.r.03)=\t\t\t0\n",
            "Target: institution.n.03\n",
            "similarity(institution.n.03, unfair.a.01)=\t\t\t0\n",
            "similarity(institution.n.03, unjust.a.02)=\t\t\t0\n",
            "similarity(institution.n.03, inequitable.a.01)=\t\t\t0\n",
            "similarity(institution.n.03, hierarchy.n.01)=\t\t\t0.09090909090909091\n",
            "similarity(institution.n.03, hierarchy.n.02)=\t\t\t0.1\n",
            "similarity(institution.n.03, be.v.01)=\t\t\t0\n",
            "similarity(institution.n.03, be.v.02)=\t\t\t0\n",
            "similarity(institution.n.03, be.v.03)=\t\t\t0\n",
            "similarity(institution.n.03, exist.v.01)=\t\t\t0\n",
            "similarity(institution.n.03, be.v.05)=\t\t\t0\n",
            "similarity(institution.n.03, equal.v.01)=\t\t\t0\n",
            "similarity(institution.n.03, constitute.v.01)=\t\t\t0\n",
            "similarity(institution.n.03, be.v.08)=\t\t\t0\n",
            "similarity(institution.n.03, embody.v.02)=\t\t\t0\n",
            "similarity(institution.n.03, be.v.10)=\t\t\t0\n",
            "similarity(institution.n.03, be.v.11)=\t\t\t0\n",
            "similarity(institution.n.03, be.v.12)=\t\t\t0\n",
            "similarity(institution.n.03, cost.v.01)=\t\t\t0\n",
            "similarity(institution.n.03, frequently.r.01)=\t\t\t0\n",
            "similarity(institution.n.03, much.r.05)=\t\t\t0\n",
            "similarity(institution.n.03, often.r.03)=\t\t\t0\n",
            "Target: initiation.n.02\n",
            "similarity(initiation.n.02, unfair.a.01)=\t\t\t0\n",
            "similarity(initiation.n.02, unjust.a.02)=\t\t\t0\n",
            "similarity(initiation.n.02, inequitable.a.01)=\t\t\t0\n",
            "similarity(initiation.n.02, hierarchy.n.01)=\t\t\t0.07142857142857142\n",
            "similarity(initiation.n.02, hierarchy.n.02)=\t\t\t0.07692307692307693\n",
            "similarity(initiation.n.02, be.v.01)=\t\t\t0\n",
            "similarity(initiation.n.02, be.v.02)=\t\t\t0\n",
            "similarity(initiation.n.02, be.v.03)=\t\t\t0\n",
            "similarity(initiation.n.02, exist.v.01)=\t\t\t0\n",
            "similarity(initiation.n.02, be.v.05)=\t\t\t0\n",
            "similarity(initiation.n.02, equal.v.01)=\t\t\t0\n",
            "similarity(initiation.n.02, constitute.v.01)=\t\t\t0\n",
            "similarity(initiation.n.02, be.v.08)=\t\t\t0\n",
            "similarity(initiation.n.02, embody.v.02)=\t\t\t0\n",
            "similarity(initiation.n.02, be.v.10)=\t\t\t0\n",
            "similarity(initiation.n.02, be.v.11)=\t\t\t0\n",
            "similarity(initiation.n.02, be.v.12)=\t\t\t0\n",
            "similarity(initiation.n.02, cost.v.01)=\t\t\t0\n",
            "similarity(initiation.n.02, frequently.r.01)=\t\t\t0\n",
            "similarity(initiation.n.02, much.r.05)=\t\t\t0\n",
            "similarity(initiation.n.02, often.r.03)=\t\t\t0\n",
            "Target: mental_hospital.n.01\n",
            "similarity(mental_hospital.n.01, unfair.a.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, unjust.a.02)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, inequitable.a.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, hierarchy.n.01)=\t\t\t0.0625\n",
            "similarity(mental_hospital.n.01, hierarchy.n.02)=\t\t\t0.06666666666666667\n",
            "similarity(mental_hospital.n.01, be.v.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, be.v.02)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, be.v.03)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, exist.v.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, be.v.05)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, equal.v.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, constitute.v.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, be.v.08)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, embody.v.02)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, be.v.10)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, be.v.11)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, be.v.12)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, cost.v.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, frequently.r.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, much.r.05)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, often.r.03)=\t\t\t0\n",
            "Context: [('several', 'a'), ('author', 'n'), ('define', 'v'), ('more', 'r')]\n",
            "Target: have.v.01\n",
            "similarity(have.v.01, several.s.01)=\t\t\t0\n",
            "similarity(have.v.01, respective.s.01)=\t\t\t0\n",
            "similarity(have.v.01, several.s.03)=\t\t\t0\n",
            "similarity(have.v.01, writer.n.01)=\t\t\t0\n",
            "similarity(have.v.01, generator.n.03)=\t\t\t0\n",
            "similarity(have.v.01, specify.v.03)=\t\t\t0\n",
            "similarity(have.v.01, define.v.02)=\t\t\t0\n",
            "similarity(have.v.01, define.v.03)=\t\t\t0\n",
            "similarity(have.v.01, define.v.04)=\t\t\t0\n",
            "similarity(have.v.01, specify.v.02)=\t\t\t0\n",
            "similarity(have.v.01, more.r.01)=\t\t\t0\n",
            "similarity(have.v.01, more.r.02)=\t\t\t0\n",
            "Target: have.v.02\n",
            "similarity(have.v.02, several.s.01)=\t\t\t0\n",
            "similarity(have.v.02, respective.s.01)=\t\t\t0\n",
            "similarity(have.v.02, several.s.03)=\t\t\t0\n",
            "similarity(have.v.02, writer.n.01)=\t\t\t0\n",
            "similarity(have.v.02, generator.n.03)=\t\t\t0\n",
            "similarity(have.v.02, specify.v.03)=\t\t\t0\n",
            "similarity(have.v.02, define.v.02)=\t\t\t0\n",
            "similarity(have.v.02, define.v.03)=\t\t\t0\n",
            "similarity(have.v.02, define.v.04)=\t\t\t0\n",
            "similarity(have.v.02, specify.v.02)=\t\t\t0\n",
            "similarity(have.v.02, more.r.01)=\t\t\t0\n",
            "similarity(have.v.02, more.r.02)=\t\t\t0\n",
            "Target: experience.v.03\n",
            "similarity(experience.v.03, several.s.01)=\t\t\t0\n",
            "similarity(experience.v.03, respective.s.01)=\t\t\t0\n",
            "similarity(experience.v.03, several.s.03)=\t\t\t0\n",
            "similarity(experience.v.03, writer.n.01)=\t\t\t0\n",
            "similarity(experience.v.03, generator.n.03)=\t\t\t0\n",
            "similarity(experience.v.03, specify.v.03)=\t\t\t0\n",
            "similarity(experience.v.03, define.v.02)=\t\t\t0\n",
            "similarity(experience.v.03, define.v.03)=\t\t\t0\n",
            "similarity(experience.v.03, define.v.04)=\t\t\t0\n",
            "similarity(experience.v.03, specify.v.02)=\t\t\t0\n",
            "similarity(experience.v.03, more.r.01)=\t\t\t0\n",
            "similarity(experience.v.03, more.r.02)=\t\t\t0\n",
            "Target: own.v.01\n",
            "similarity(own.v.01, several.s.01)=\t\t\t0\n",
            "similarity(own.v.01, respective.s.01)=\t\t\t0\n",
            "similarity(own.v.01, several.s.03)=\t\t\t0\n",
            "similarity(own.v.01, writer.n.01)=\t\t\t0\n",
            "similarity(own.v.01, generator.n.03)=\t\t\t0\n",
            "similarity(own.v.01, specify.v.03)=\t\t\t0\n",
            "similarity(own.v.01, define.v.02)=\t\t\t0\n",
            "similarity(own.v.01, define.v.03)=\t\t\t0\n",
            "similarity(own.v.01, define.v.04)=\t\t\t0\n",
            "similarity(own.v.01, specify.v.02)=\t\t\t0\n",
            "similarity(own.v.01, more.r.01)=\t\t\t0\n",
            "similarity(own.v.01, more.r.02)=\t\t\t0\n",
            "Target: get.v.03\n",
            "similarity(get.v.03, several.s.01)=\t\t\t0\n",
            "similarity(get.v.03, respective.s.01)=\t\t\t0\n",
            "similarity(get.v.03, several.s.03)=\t\t\t0\n",
            "similarity(get.v.03, writer.n.01)=\t\t\t0\n",
            "similarity(get.v.03, generator.n.03)=\t\t\t0\n",
            "similarity(get.v.03, specify.v.03)=\t\t\t0\n",
            "similarity(get.v.03, define.v.02)=\t\t\t0\n",
            "similarity(get.v.03, define.v.03)=\t\t\t0\n",
            "similarity(get.v.03, define.v.04)=\t\t\t0\n",
            "similarity(get.v.03, specify.v.02)=\t\t\t0\n",
            "similarity(get.v.03, more.r.01)=\t\t\t0\n",
            "similarity(get.v.03, more.r.02)=\t\t\t0\n",
            "Target: consume.v.02\n",
            "similarity(consume.v.02, several.s.01)=\t\t\t0\n",
            "similarity(consume.v.02, respective.s.01)=\t\t\t0\n",
            "similarity(consume.v.02, several.s.03)=\t\t\t0\n",
            "similarity(consume.v.02, writer.n.01)=\t\t\t0\n",
            "similarity(consume.v.02, generator.n.03)=\t\t\t0\n",
            "similarity(consume.v.02, specify.v.03)=\t\t\t0\n",
            "similarity(consume.v.02, define.v.02)=\t\t\t0\n",
            "similarity(consume.v.02, define.v.03)=\t\t\t0\n",
            "similarity(consume.v.02, define.v.04)=\t\t\t0\n",
            "similarity(consume.v.02, specify.v.02)=\t\t\t0\n",
            "similarity(consume.v.02, more.r.01)=\t\t\t0\n",
            "similarity(consume.v.02, more.r.02)=\t\t\t0\n",
            "Target: have.v.07\n",
            "similarity(have.v.07, several.s.01)=\t\t\t0\n",
            "similarity(have.v.07, respective.s.01)=\t\t\t0\n",
            "similarity(have.v.07, several.s.03)=\t\t\t0\n",
            "similarity(have.v.07, writer.n.01)=\t\t\t0\n",
            "similarity(have.v.07, generator.n.03)=\t\t\t0\n",
            "similarity(have.v.07, specify.v.03)=\t\t\t0\n",
            "similarity(have.v.07, define.v.02)=\t\t\t0\n",
            "similarity(have.v.07, define.v.03)=\t\t\t0\n",
            "similarity(have.v.07, define.v.04)=\t\t\t0\n",
            "similarity(have.v.07, specify.v.02)=\t\t\t0\n",
            "similarity(have.v.07, more.r.01)=\t\t\t0\n",
            "similarity(have.v.07, more.r.02)=\t\t\t0\n",
            "Target: hold.v.03\n",
            "similarity(hold.v.03, several.s.01)=\t\t\t0\n",
            "similarity(hold.v.03, respective.s.01)=\t\t\t0\n",
            "similarity(hold.v.03, several.s.03)=\t\t\t0\n",
            "similarity(hold.v.03, writer.n.01)=\t\t\t0\n",
            "similarity(hold.v.03, generator.n.03)=\t\t\t0\n",
            "similarity(hold.v.03, specify.v.03)=\t\t\t0\n",
            "similarity(hold.v.03, define.v.02)=\t\t\t0\n",
            "similarity(hold.v.03, define.v.03)=\t\t\t0\n",
            "similarity(hold.v.03, define.v.04)=\t\t\t0\n",
            "similarity(hold.v.03, specify.v.02)=\t\t\t0\n",
            "similarity(hold.v.03, more.r.01)=\t\t\t0\n",
            "similarity(hold.v.03, more.r.02)=\t\t\t0\n",
            "Target: have.v.09\n",
            "similarity(have.v.09, several.s.01)=\t\t\t0\n",
            "similarity(have.v.09, respective.s.01)=\t\t\t0\n",
            "similarity(have.v.09, several.s.03)=\t\t\t0\n",
            "similarity(have.v.09, writer.n.01)=\t\t\t0\n",
            "similarity(have.v.09, generator.n.03)=\t\t\t0\n",
            "similarity(have.v.09, specify.v.03)=\t\t\t0\n",
            "similarity(have.v.09, define.v.02)=\t\t\t0\n",
            "similarity(have.v.09, define.v.03)=\t\t\t0\n",
            "similarity(have.v.09, define.v.04)=\t\t\t0\n",
            "similarity(have.v.09, specify.v.02)=\t\t\t0\n",
            "similarity(have.v.09, more.r.01)=\t\t\t0\n",
            "similarity(have.v.09, more.r.02)=\t\t\t0\n",
            "Target: have.v.10\n",
            "similarity(have.v.10, several.s.01)=\t\t\t0\n",
            "similarity(have.v.10, respective.s.01)=\t\t\t0\n",
            "similarity(have.v.10, several.s.03)=\t\t\t0\n",
            "similarity(have.v.10, writer.n.01)=\t\t\t0\n",
            "similarity(have.v.10, generator.n.03)=\t\t\t0\n",
            "similarity(have.v.10, specify.v.03)=\t\t\t0\n",
            "similarity(have.v.10, define.v.02)=\t\t\t0\n",
            "similarity(have.v.10, define.v.03)=\t\t\t0\n",
            "similarity(have.v.10, define.v.04)=\t\t\t0\n",
            "similarity(have.v.10, specify.v.02)=\t\t\t0\n",
            "similarity(have.v.10, more.r.01)=\t\t\t0\n",
            "similarity(have.v.10, more.r.02)=\t\t\t0\n",
            "Target: have.v.11\n",
            "similarity(have.v.11, several.s.01)=\t\t\t0\n",
            "similarity(have.v.11, respective.s.01)=\t\t\t0\n",
            "similarity(have.v.11, several.s.03)=\t\t\t0\n",
            "similarity(have.v.11, writer.n.01)=\t\t\t0\n",
            "similarity(have.v.11, generator.n.03)=\t\t\t0\n",
            "similarity(have.v.11, specify.v.03)=\t\t\t0\n",
            "similarity(have.v.11, define.v.02)=\t\t\t0\n",
            "similarity(have.v.11, define.v.03)=\t\t\t0\n",
            "similarity(have.v.11, define.v.04)=\t\t\t0\n",
            "similarity(have.v.11, specify.v.02)=\t\t\t0\n",
            "similarity(have.v.11, more.r.01)=\t\t\t0\n",
            "similarity(have.v.11, more.r.02)=\t\t\t0\n",
            "Target: have.v.12\n",
            "similarity(have.v.12, several.s.01)=\t\t\t0\n",
            "similarity(have.v.12, respective.s.01)=\t\t\t0\n",
            "similarity(have.v.12, several.s.03)=\t\t\t0\n",
            "similarity(have.v.12, writer.n.01)=\t\t\t0\n",
            "similarity(have.v.12, generator.n.03)=\t\t\t0\n",
            "similarity(have.v.12, specify.v.03)=\t\t\t0.25\n",
            "similarity(have.v.12, define.v.02)=\t\t\t0\n",
            "similarity(have.v.12, define.v.03)=\t\t\t0\n",
            "similarity(have.v.12, define.v.04)=\t\t\t0\n",
            "similarity(have.v.12, specify.v.02)=\t\t\t0\n",
            "similarity(have.v.12, more.r.01)=\t\t\t0\n",
            "similarity(have.v.12, more.r.02)=\t\t\t0\n",
            "Target: induce.v.02\n",
            "similarity(induce.v.02, several.s.01)=\t\t\t0\n",
            "similarity(induce.v.02, respective.s.01)=\t\t\t0\n",
            "similarity(induce.v.02, several.s.03)=\t\t\t0\n",
            "similarity(induce.v.02, writer.n.01)=\t\t\t0\n",
            "similarity(induce.v.02, generator.n.03)=\t\t\t0\n",
            "similarity(induce.v.02, specify.v.03)=\t\t\t0\n",
            "similarity(induce.v.02, define.v.02)=\t\t\t0\n",
            "similarity(induce.v.02, define.v.03)=\t\t\t0\n",
            "similarity(induce.v.02, define.v.04)=\t\t\t0\n",
            "similarity(induce.v.02, specify.v.02)=\t\t\t0\n",
            "similarity(induce.v.02, more.r.01)=\t\t\t0\n",
            "similarity(induce.v.02, more.r.02)=\t\t\t0\n",
            "Target: accept.v.02\n",
            "similarity(accept.v.02, several.s.01)=\t\t\t0\n",
            "similarity(accept.v.02, respective.s.01)=\t\t\t0\n",
            "similarity(accept.v.02, several.s.03)=\t\t\t0\n",
            "similarity(accept.v.02, writer.n.01)=\t\t\t0\n",
            "similarity(accept.v.02, generator.n.03)=\t\t\t0\n",
            "similarity(accept.v.02, specify.v.03)=\t\t\t0\n",
            "similarity(accept.v.02, define.v.02)=\t\t\t0\n",
            "similarity(accept.v.02, define.v.03)=\t\t\t0\n",
            "similarity(accept.v.02, define.v.04)=\t\t\t0\n",
            "similarity(accept.v.02, specify.v.02)=\t\t\t0\n",
            "similarity(accept.v.02, more.r.01)=\t\t\t0\n",
            "similarity(accept.v.02, more.r.02)=\t\t\t0\n",
            "Target: receive.v.01\n",
            "similarity(receive.v.01, several.s.01)=\t\t\t0\n",
            "similarity(receive.v.01, respective.s.01)=\t\t\t0\n",
            "similarity(receive.v.01, several.s.03)=\t\t\t0\n",
            "similarity(receive.v.01, writer.n.01)=\t\t\t0\n",
            "similarity(receive.v.01, generator.n.03)=\t\t\t0\n",
            "similarity(receive.v.01, specify.v.03)=\t\t\t0\n",
            "similarity(receive.v.01, define.v.02)=\t\t\t0\n",
            "similarity(receive.v.01, define.v.03)=\t\t\t0\n",
            "similarity(receive.v.01, define.v.04)=\t\t\t0\n",
            "similarity(receive.v.01, specify.v.02)=\t\t\t0\n",
            "similarity(receive.v.01, more.r.01)=\t\t\t0\n",
            "similarity(receive.v.01, more.r.02)=\t\t\t0\n",
            "Target: suffer.v.02\n",
            "similarity(suffer.v.02, several.s.01)=\t\t\t0\n",
            "similarity(suffer.v.02, respective.s.01)=\t\t\t0\n",
            "similarity(suffer.v.02, several.s.03)=\t\t\t0\n",
            "similarity(suffer.v.02, writer.n.01)=\t\t\t0\n",
            "similarity(suffer.v.02, generator.n.03)=\t\t\t0\n",
            "similarity(suffer.v.02, specify.v.03)=\t\t\t0\n",
            "similarity(suffer.v.02, define.v.02)=\t\t\t0\n",
            "similarity(suffer.v.02, define.v.03)=\t\t\t0\n",
            "similarity(suffer.v.02, define.v.04)=\t\t\t0\n",
            "similarity(suffer.v.02, specify.v.02)=\t\t\t0\n",
            "similarity(suffer.v.02, more.r.01)=\t\t\t0\n",
            "similarity(suffer.v.02, more.r.02)=\t\t\t0\n",
            "Target: have.v.17\n",
            "similarity(have.v.17, several.s.01)=\t\t\t0\n",
            "similarity(have.v.17, respective.s.01)=\t\t\t0\n",
            "similarity(have.v.17, several.s.03)=\t\t\t0\n",
            "similarity(have.v.17, writer.n.01)=\t\t\t0\n",
            "similarity(have.v.17, generator.n.03)=\t\t\t0\n",
            "similarity(have.v.17, specify.v.03)=\t\t\t0\n",
            "similarity(have.v.17, define.v.02)=\t\t\t0\n",
            "similarity(have.v.17, define.v.03)=\t\t\t0\n",
            "similarity(have.v.17, define.v.04)=\t\t\t0\n",
            "similarity(have.v.17, specify.v.02)=\t\t\t0\n",
            "similarity(have.v.17, more.r.01)=\t\t\t0\n",
            "similarity(have.v.17, more.r.02)=\t\t\t0\n",
            "Target: give_birth.v.01\n",
            "similarity(give_birth.v.01, several.s.01)=\t\t\t0\n",
            "similarity(give_birth.v.01, respective.s.01)=\t\t\t0\n",
            "similarity(give_birth.v.01, several.s.03)=\t\t\t0\n",
            "similarity(give_birth.v.01, writer.n.01)=\t\t\t0\n",
            "similarity(give_birth.v.01, generator.n.03)=\t\t\t0\n",
            "similarity(give_birth.v.01, specify.v.03)=\t\t\t0\n",
            "similarity(give_birth.v.01, define.v.02)=\t\t\t0\n",
            "similarity(give_birth.v.01, define.v.03)=\t\t\t0\n",
            "similarity(give_birth.v.01, define.v.04)=\t\t\t0\n",
            "similarity(give_birth.v.01, specify.v.02)=\t\t\t0\n",
            "similarity(give_birth.v.01, more.r.01)=\t\t\t0\n",
            "similarity(give_birth.v.01, more.r.02)=\t\t\t0\n",
            "Target: take.v.35\n",
            "similarity(take.v.35, several.s.01)=\t\t\t0\n",
            "similarity(take.v.35, respective.s.01)=\t\t\t0\n",
            "similarity(take.v.35, several.s.03)=\t\t\t0\n",
            "similarity(take.v.35, writer.n.01)=\t\t\t0\n",
            "similarity(take.v.35, generator.n.03)=\t\t\t0\n",
            "similarity(take.v.35, specify.v.03)=\t\t\t0\n",
            "similarity(take.v.35, define.v.02)=\t\t\t0\n",
            "similarity(take.v.35, define.v.03)=\t\t\t0\n",
            "similarity(take.v.35, define.v.04)=\t\t\t0\n",
            "similarity(take.v.35, specify.v.02)=\t\t\t0\n",
            "similarity(take.v.35, more.r.01)=\t\t\t0\n",
            "similarity(take.v.35, more.r.02)=\t\t\t0\n",
            "Context: [('more', 'r'), ('specifically', 'r'), ('base', 'v'), ('hierarchical', 'a')]\n",
            "Target: institution.n.01\n",
            "similarity(institution.n.01, more.r.01)=\t\t\t0\n",
            "similarity(institution.n.01, more.r.02)=\t\t\t0\n",
            "similarity(institution.n.01, specifically.r.01)=\t\t\t0\n",
            "similarity(institution.n.01, establish.v.08)=\t\t\t0\n",
            "similarity(institution.n.01, base.v.02)=\t\t\t0\n",
            "similarity(institution.n.01, free-base.v.01)=\t\t\t0\n",
            "similarity(institution.n.01, hierarchical.a.01)=\t\t\t0\n",
            "Target: institution.n.02\n",
            "similarity(institution.n.02, more.r.01)=\t\t\t0\n",
            "similarity(institution.n.02, more.r.02)=\t\t\t0\n",
            "similarity(institution.n.02, specifically.r.01)=\t\t\t0\n",
            "similarity(institution.n.02, establish.v.08)=\t\t\t0\n",
            "similarity(institution.n.02, base.v.02)=\t\t\t0\n",
            "similarity(institution.n.02, free-base.v.01)=\t\t\t0\n",
            "similarity(institution.n.02, hierarchical.a.01)=\t\t\t0\n",
            "Target: institution.n.03\n",
            "similarity(institution.n.03, more.r.01)=\t\t\t0\n",
            "similarity(institution.n.03, more.r.02)=\t\t\t0\n",
            "similarity(institution.n.03, specifically.r.01)=\t\t\t0\n",
            "similarity(institution.n.03, establish.v.08)=\t\t\t0\n",
            "similarity(institution.n.03, base.v.02)=\t\t\t0\n",
            "similarity(institution.n.03, free-base.v.01)=\t\t\t0\n",
            "similarity(institution.n.03, hierarchical.a.01)=\t\t\t0\n",
            "Target: initiation.n.02\n",
            "similarity(initiation.n.02, more.r.01)=\t\t\t0\n",
            "similarity(initiation.n.02, more.r.02)=\t\t\t0\n",
            "similarity(initiation.n.02, specifically.r.01)=\t\t\t0\n",
            "similarity(initiation.n.02, establish.v.08)=\t\t\t0\n",
            "similarity(initiation.n.02, base.v.02)=\t\t\t0\n",
            "similarity(initiation.n.02, free-base.v.01)=\t\t\t0\n",
            "similarity(initiation.n.02, hierarchical.a.01)=\t\t\t0\n",
            "Target: mental_hospital.n.01\n",
            "similarity(mental_hospital.n.01, more.r.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, more.r.02)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, specifically.r.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, establish.v.08)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, base.v.02)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, free-base.v.01)=\t\t\t0\n",
            "similarity(mental_hospital.n.01, hierarchical.a.01)=\t\t\t0\n",
            "CPU times: user 173 ms, sys: 35.9 ms, total: 209 ms\n",
            "Wall time: 171 ms\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HXqKBXyRYtH_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 629
        },
        "outputId": "42dcb8c7-389f-4304-95c7-4808f3b33277"
      },
      "cell_type": "code",
      "source": [
        "a_sub_deambiguated"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anarchism',\n",
              " 'be',\n",
              " 'political',\n",
              " 'philosophy',\n",
              " 'advocate',\n",
              " 'self',\n",
              " 'regulate.v.02',\n",
              " 'society',\n",
              " 'base',\n",
              " 'voluntary',\n",
              " 'cooperative',\n",
              " 'institution.n.01',\n",
              " 'reject',\n",
              " 'unjust',\n",
              " 'hierarchy',\n",
              " 'institution.n.01',\n",
              " 'be',\n",
              " 'often',\n",
              " 'describe',\n",
              " 'stateless',\n",
              " 'society',\n",
              " 'several',\n",
              " 'author',\n",
              " 'have.v.12',\n",
              " 'define',\n",
              " 'more',\n",
              " 'specifically',\n",
              " 'institution.n.01',\n",
              " 'base',\n",
              " 'hierarchical',\n",
              " 'free',\n",
              " 'association',\n",
              " 'anarchism',\n",
              " 'hold',\n",
              " 'capitalism',\n",
              " 'state']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "metadata": {
        "id": "Zrg-_SwOYNve",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Comparación normal vs. desambiguado\n",
        "\n",
        "Se puede comparar la muestra del artículo previo y posterior a la desambiguación:"
      ]
    },
    {
      "metadata": {
        "id": "ULePJxlKUhub",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "outputId": "4099ffde-c875-4c6f-c463-4627d92780c4"
      },
      "cell_type": "code",
      "source": [
        "print(\"ANTES | DESPUÉS\")\n",
        "for w, x in zip(a_sub, a_sub_deambiguated):\n",
        "  print(w.split('/')[0], '|', x)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ANTES | DESPUÉS\n",
            "anarchism | anarchism\n",
            "be | be\n",
            "political | political\n",
            "philosophy | philosophy\n",
            "advocate | advocate\n",
            "self | self\n",
            "govern | regulate.v.02\n",
            "society | society\n",
            "base | base\n",
            "voluntary | voluntary\n",
            "cooperative | cooperative\n",
            "institution | institution.n.01\n",
            "reject | reject\n",
            "unjust | unjust\n",
            "hierarchy | hierarchy\n",
            "institution | institution.n.01\n",
            "be | be\n",
            "often | often\n",
            "describe | describe\n",
            "stateless | stateless\n",
            "society | society\n",
            "several | several\n",
            "author | author\n",
            "have | have.v.12\n",
            "define | define\n",
            "more | more\n",
            "specifically | specifically\n",
            "institution | institution.n.01\n",
            "base | base\n",
            "hierarchical | hierarchical\n",
            "free | free\n",
            "association | association\n",
            "anarchism | anarchism\n",
            "hold | hold\n",
            "capitalism | capitalism\n",
            "state | state\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xkw0KwFgZ5qm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Conclusión\n",
        "\n",
        "El costo de desambiguar palabras es alto, pero se puede reducir en gran medida realizando preprocesado de manera *offline*: tokenización, remoción de *stop words*, lematización y *PoS-tagging*.\n",
        "\n",
        "Es necesario que el modelo preprocesador pueda determinar autónomamente aquellas palabras a desambiguar, pero en esta iteración no se logró ni fue una gran prioridad. Lo será durante las siguientes iteraciones, donde los casos de prueba tendrán mayor tamaño, y donde es necesaria la independencia de los modelos con respecto al *input* manual de humanos.\n",
        "\n",
        "Si bien el trabajo desarrollado en esta iteración no alcanzó a cubrir las tareas de validación, como el entrenamiento de modelos Word2vec basados en el corpus desambiguado, y las *downstream tasks* como recuperación de información, se sostiene que éstas pueden ser razonablemente postergadas en favor de lograr refinaciones en el proceso de desambiguación (tareas que consumen tiempo, pues implican la carga y procesamiento de grandes volúmenes de información).\n",
        "\n"
      ]
    }
  ]
}